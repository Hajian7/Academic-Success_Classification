{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837b1ca1-5d87-432b-aefd-d71b1405a17c",
   "metadata": {},
   "source": [
    "We have more than 76.5 synthetic data points available for constructing a model. This model will predict whether individuals have graduated, dropped out, or are still enrolled, based on specific features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f718644-8cfe-4aa2-afa5-b2d2bed80c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical\n",
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "\n",
    "# Model libraries\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "\n",
    "# Loading the data\n",
    "file_path_train = '/kaggle/input/playground-series-s4e6/train.csv'\n",
    "file_path_test = '/kaggle/input/playground-series-s4e6/test.csv'\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(file_path_train)\n",
    "df_test = pd.read_csv(file_path_test)\n",
    "\n",
    "# Dropping the first column\n",
    "submission_id = df_test['id'].reset_index(drop=True)\n",
    "df_train = df_train.drop(columns='id')\n",
    "df_test = df_test.drop(columns='id')\n",
    "\n",
    "# Shape of train and test data\n",
    "print(\"train shape: \", df_train.shape)\n",
    "print(\"test shape: \", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255a853-3236-4fc8-88bd-4809f41161fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00ef20-8da2-4851-ba68-b95a36c5cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea77ce-6d36-4f96-a5b6-55254e993aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59540e2f-da9f-46a1-94cd-cf87a618b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the datatype of the categorial data\n",
    "category_columns_train = [0,1,3,4,5,7,8,9,10,11,13,14,15,16,17,18,20,36]\n",
    "for col in category_columns_train:\n",
    "    df_train.iloc[:, col] = df_train.iloc[:, col].astype('str').astype('category')\n",
    "\n",
    "category_columns_test = [0,1,3,4,5,7,8,9,10,11,13,14,15,16,17,18,20]\n",
    "for col in category_columns_test:\n",
    "    df_test.iloc[:, col] = df_test.iloc[:, col].astype('str').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829fa49-1979-4427-9801-c8018b41a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b046f6-4423-44cd-bf0f-efbd696b3485",
   "metadata": {},
   "source": [
    "Let's look at each variable individually to understand the distribution of data in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e50c2b-a826-428e-a045-a11be7806e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_columns(df, columns):\n",
    "    \"\"\"\n",
    "    Visualizes the minimum, maximum, histogram, box plot, and density plot for specified columns,\n",
    "    each in a single row.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the data.\n",
    "    columns (list of str): List of column names to visualize.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            # Create a figure with 1 row and 4 columns of subplots\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(20, 4))  \n",
    "\n",
    "            # Min and Max visualization\n",
    "            min_val = df[column].min()\n",
    "            max_val = df[column].max()\n",
    "            mean_val = df[column].mean().round(4)\n",
    "            median_val = df[column].median()\n",
    "            mode_val = df[column].mode()[0]\n",
    "            std_val = round(df[column].std(), 4)\n",
    "            axs[0].text(0.5, 0.5, f'Min: {min_val}\\nMax: {max_val}\\nMean: {mean_val}\\nMedian: {median_val}\\nMode: {mode_val}\\nSTD: {std_val}', horizontalalignment='center', \n",
    "                        verticalalignment='center', fontsize=20, transform=axs[0].transAxes)\n",
    "            axs[0].set_title(f'Min/Max/Mean of {column}', fontsize=15)\n",
    "            axs[0].axis('off')  # Hide axes\n",
    "\n",
    "            # Histogram\n",
    "            axs[1].hist(df[column], bins=30, color=plt.cm.viridis(0.9), edgecolor='black')\n",
    "            axs[1].set_title(f'Histogram of {column}', fontsize=15)\n",
    "\n",
    "            # Box plot\n",
    "            sns.boxplot(x=df[column], ax=axs[2], palette='viridis')\n",
    "            axs[0].set_title(f'Min/Max/Mean of {column}')\n",
    "            axs[2].set_title(f'Box Plot of {column}', fontsize=15)\n",
    "\n",
    "            # Density Plot\n",
    "            sns.kdeplot(df[column], ax=axs[3], fill=True, color=plt.cm.viridis(0.1))\n",
    "            axs[3].set_title(f'Density Plot of {column}', fontsize=15)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "visualize_columns(df_train, df_train.select_dtypes(include=['float64', 'int64']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ea141-0d6e-4484-aa1e-3f49d4b9895b",
   "metadata": {},
   "source": [
    "Let's perform a spot check on a few of the features with unusual box plots to determine if additional data transformations are needed to properly interpret these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602068d-0c8f-4d68-a414-5ad92197f5b8",
   "metadata": {},
   "source": [
    "**Explore the relationship between categorical variable and target values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ced2e1-6ed9-4de8-88de-bb59cb0520f8",
   "metadata": {},
   "source": [
    "Let’s implement a Chi-Square test of independence as an example to check the association between each categorical feature and the target.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227adfe-8f3c-40ea-8b71-be1492e37929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and labels\n",
    "X = df_train.drop(columns='Target')\n",
    "y = df_train['Target']\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features= X.select_dtypes(include='category').columns\n",
    "numerical_features= X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Find p-values\n",
    "p_values = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    contingency_table = pd.crosstab(df_train[col], y)\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    p_values[col] = p\n",
    "\n",
    "for col, p in p_values.items():\n",
    "    print(f\"P-value for {col} and target: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98da70c-cac1-4ec6-890e-f99f3bb6a97b",
   "metadata": {},
   "source": [
    "**Interpretation:**\r\n",
    "\r\n",
    "Null hypothesis: There is no association between the two variables.\r\n",
    "\r\n",
    "Low p-values (<0.05): Strong evidence against the null hypothesis, suggesting a significant association between the feature and the target.\r\n",
    "High p-values: Weak evidence against the null hypothesis, no significant association found.\r\n",
    "\r\n",
    "So the features with less than 0.05 p_value has statistically significant association with the target values. This means that these features likely provide some information about the target categories.\r\n",
    "\r\n",
    "Marital Status\r\n",
    "\r\n",
    "Application Mode\r\n",
    "\r\n",
    "Course\r\n",
    "\r\n",
    "Daytime/Evening Attendance (though this is not zero, it's a very small number, indicating significance)\r\n",
    "\r\n",
    "Previous Qualification\r\n",
    "\r\n",
    "Nacionality\r\n",
    "\r\n",
    "Mother's Qualification\r\n",
    "\r\n",
    "Father's Qualification\r\n",
    "\r\n",
    "Mother's Occupation\r\n",
    "\r\n",
    "Father's Occupation\r\n",
    "\r\n",
    "Displaced\r\n",
    "\r\n",
    "Debtor\r\n",
    "\r\n",
    "Tuition Fees Up to Date\r\n",
    "\r\n",
    "Gender\r\n",
    "\r\n",
    "Scholarship Holder\r\n",
    "\r\n",
    "Below features have high p-values (greater than 0.05), suggesting that there is no significant association with the target values under the common significance level. This implies that these features do not provide reliable information about the target categories:\r\n",
    "\r\n",
    "Educational Special Needs (p-value: 0.8900215041644881)\r\n",
    "International (p-value: 0.7257456838845804)\r\n",
    "Lets calculate Cramér's V, which measures the strength of association between two categorical variables, we'll use the Chi-Square statistic derived from the contingency table of each feature with the target. Cramér's V provides a value between 0 and 1, where 0 indicates no association and 1 indicates a perfect association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8506a6-d0f0-4b87-a3ec-c5219b30cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(chi2, n, k, r):\n",
    "    return np.sqrt(chi2 / n / min(k - 1, r - 1))\n",
    "\n",
    "# Dictionary to store Cramér's V values\n",
    "cramers_v_results = {}\n",
    "for col in categorical_features:\n",
    "    table = pd.crosstab(df_train[col], y)\n",
    "    chi2, p, dof, expected = chi2_contingency(table)\n",
    "    n = table.sum().sum()  # Total observations\n",
    "    r, k = table.shape\n",
    "    v = cramers_v(chi2, n, k, r)\n",
    "    cramers_v_results[col] = v\n",
    "\n",
    "# Print Cramér's V results\n",
    "for feature, v in cramers_v_results.items():\n",
    "    print(f\"Cramér's V for {feature} and target: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f0c64-799d-44c4-a465-2cf0878dce7c",
   "metadata": {},
   "source": [
    "**Interpretation:** \n",
    "\n",
    "\n",
    "High Association: \n",
    "* Tuition Fees Up to Date: 0.4472 - This feature has the highest association among the features listed, making it a strong predictor.\n",
    "* Scholarship Holder: 0.4064 - Also shows a strong association, suggesting it significantly influences or relates to the target.\n",
    "* Course: 0.3364 - This is another strong indicator, reflecting its relevance to the target.\n",
    "* Gender: 0.3302 - Significant association, useful for predicting the target.\n",
    "\n",
    "Moderate Association: \n",
    "- Application Mode: 0.2925 - Shows a moderate relationship with the target.\n",
    "- Debtor: 0.2589 - Relatively moderate, indicating some predictive power.\n",
    "- Previous Qualification: 0.1916 - Offers some insight but not as strong as the top predictors.\n",
    "- Mother's Qualification, Mother's Occupation, Father's Qualification, Father's Occupation: These features show similar moderate associations (ranging from 0.1636 to 0.1765), suggesting they hold some predictive value. \n",
    "\n",
    "**Consideration** Focus on features with moderate to high Cramér's V values for model building, as these are likely to provide the most predictive power. Consider disregarding or deprioritizing features with very low Cramér's V values in predictive modeling, as they may not contribute significantly to model accuracy. Further investigate combinations of features or interactions that could enhance model performance, especially where moderate associations exist.iations exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738ae19-44c3-4476-9acc-3337314b6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ANOVA for multi-class target variable\n",
    "correlation_results = {}\n",
    "for feature in numerical_features:\n",
    "    groups = [df_train[feature][y == category].values for category in y.unique()]\n",
    "    f_value, p_value = f_oneway(*groups)\n",
    "    correlation_results[feature] = {'F-value': f_value, 'p-value': p_value}\n",
    "\n",
    "# Convert the results to a DataFrame for better visualization\n",
    "correlation_df = pd.DataFrame(correlation_results).T\n",
    "print(correlation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8d5df-5ce1-4296-a3b8-5edbb2347faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of the correlation matrix\n",
    "\n",
    "correlation_matrix=df_train[numerical_features].corr()\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af33362-511c-4efc-82e3-0c7fd57ef92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cramér's V statistic for two categorical variables.\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1)) / (n-1))\n",
    "    rcorr = r - ((r-1)**2) / (n-1)\n",
    "    kcorr = k - ((k-1)**2) / (n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "cramers_v_matrix = pd.DataFrame(index=categorical_features, columns=categorical_features)\n",
    "\n",
    "for col1 in categorical_features:\n",
    "    for col2 in categorical_features:\n",
    "        if col1 == col2:\n",
    "            cramers_v_matrix.loc[col1, col2] = 1.0\n",
    "        else:\n",
    "            cramers_v_matrix.loc[col1, col2] = cramers_v(df_train[col1], df_train[col2])\n",
    "\n",
    "# Convert the matrix to numeric for heatmap plotting\n",
    "cramers_v_matrix = cramers_v_matrix.astype(float)\n",
    "\n",
    "# Heatmap of the correlation\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cramers_v_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title(\"Cramér's V Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853da015-dea1-4646-8b91-36184723a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling (voting)\n",
    "\n",
    "# Map class labels to numerical values, needed for XGBoost \n",
    "label_mapping = {'Dropout': 0, 'Enrolled': 1, 'Graduate': 2}\n",
    "yv = y.map(label_mapping)\n",
    "\n",
    "# categorical features needed for CatBoost\n",
    "cat_columns = categorical_features.tolist()\n",
    "\n",
    "\n",
    "# Parameters of models \n",
    "cb_params = {'learning_rate': 0.12146914273449388, 'iterations': 2656, 'depth': 3, 'l2_leaf_reg': 0.8104011685674616, \n",
    "                  'border_count': 298, 'random_strength': 0.9482384798609886, 'bagging_temperature': 0.0814623508183759} \n",
    "lgbm_params = {'learning_rate': 0.13002653030773764, 'num_leaves': 46, 'max_depth': 45, 'min_data_in_leaf': 72, 'feature_fraction': 0.34580039317123207, \n",
    "               'bagging_fraction': 0.9658924212153982}\n",
    "xgb_params = {'n_estimators': 425, 'learning_rate': 0.04261325690824416, 'max_depth': 6, 'min_child_weight': 7, 'gamma': 0.04543919342670777, \n",
    "                      'subsample': 0.9549858251941967, 'colsample_bytree': 0.7217267304024939}\n",
    "\n",
    "# models \n",
    "catboost_model = CatBoostClassifier(random_state=4, cat_features=cat_columns, verbose=False, **cb_params)\n",
    "lgbm_model = LGBMClassifier(verbose=0, random_state=5, **lgbm_params)\n",
    "xgb_model = XGBClassifier(verbose=0, random_state=2, enable_categorical=True, **xgb_params)\n",
    "\n",
    "\n",
    "# Create a VotingClassifier ensemble\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('catboost', catboost_model), ('lgbm', lgbm_model)], ('xgb', xgb_model)],\n",
    "    voting='soft'  # 'soft' uses predicted probabilities, 'hard' uses predicted class labels\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the VotingClassifier on the training data\n",
    "model = voting_clf\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores = cross_val_score(model, X, yv, scoring='accuracy', cv=kf, n_jobs=-1)\n",
    "\n",
    "print(f\"Accuracy: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5bd6e-9263-4a31-8bb9-42ebee7c113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: confusion matrix\n",
    "\n",
    "# Initialize lists to store true and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the validation data\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Append the true and predicted labels to the lists\n",
    "    true_labels.extend(y_val)\n",
    "    predicted_labels.extend(y_val_pred)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=0)[np.newaxis,:] * 100\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percentage)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47404275-e696-45a7-9540-30ff52c08147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test dataset\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "predictions = model.predict(df_test)\n",
    "\n",
    "# Reshape predictions array to be 1-dimensional\n",
    "predictions_flat = predictions.flatten()\n",
    "\n",
    "# Prepare the submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'id': submission_id,\n",
    "    'Target': predictions_flat\n",
    "})\n",
    "\n",
    "print('Submission head:', submission.head(10))\n",
    "\n",
    "# Save the submission dataframe to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
