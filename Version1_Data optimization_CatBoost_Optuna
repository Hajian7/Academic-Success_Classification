{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73290,"databundleVersionId":8710574,"sourceType":"competition"}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sajjadhajian/version1-data-optimization-catboost-optuna?scriptVersionId=180934561\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport optuna\nimport logging\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\n\n# Loading the data\nfile_path_train = '/kaggle/input/playground-series-s4e6/train.csv'\nfile_path_test = '/kaggle/input/playground-series-s4e6/test.csv'\n\ndf_train = pd.read_csv(file_path_train)\ndf_test = pd.read_csv(file_path_test)\n\nsubmission_id = df_test['id'].reset_index(drop=True)\n\n# Dropping the first column\ndf_train = df_train.drop(columns='id')\ndf_test = df_test.drop(columns='id')\n\n# Converting the float columns to numbers between -127 and 127 to convert to int8\ndef normalize_to_int8(series):\n    normalized = (series - series.min()) * (254 / (series.max() - series.min())) - 127\n    return normalized.astype('int8')\n\ncolumns_to_normalize = [6,12,25,31,33,34,35]\nfor col in columns_to_normalize:\n    df_train.iloc[:, col] = normalize_to_int8(df_train.iloc[:, col])\n    df_test.iloc[:, col] = normalize_to_int8(df_test.iloc[:, col])\n\n    \n# Creating empty dataframes with int8 data types to optimize the data\ndf_train_opt = pd.DataFrame(0, index=df_train.index, columns=df_train.columns, dtype='int8')\ndf_test_opt = pd.DataFrame(0, index=df_test.index, columns=df_test.columns, dtype='int8')\n\n# Copying the data to new dataframes\nfor col in df_train.columns[:-1]:\n    df_train_opt[col] = df_train[col].astype('int8')\ndf_train_opt['Target'] = df_train['Target'].astype('str').astype('category')\n\nfor col in df_test.columns:\n    df_test_opt[col] = df_test[col].astype('int8')\n\n# Updating the datatype of the categorial data\ncategory_columns_train = [0,1,3,4,5,7,8,9,10,11,13,14,15,16,17,18,20,36]\nfor col in category_columns_train:\n    df_train_opt.iloc[:, col] = df_train_opt.iloc[:, col].astype('str').astype('category')\n\ncategory_columns_test = [0,1,3,4,5,7,8,9,10,11,13,14,15,16,17,18,20]\nfor col in category_columns_test:\n    df_test_opt.iloc[:, col] = df_test_opt.iloc[:, col].astype('str').astype('category')\n\n# Checking the datatypes    \nprint(df_train_opt.dtypes)\nprint(df_test_opt.dtypes)\n\n# Saving new dataframes to CSV\ndf_train_opt.to_csv('train_opt.csv', index=False)\ndf_test_opt.to_csv('test_opt.csv', index=False)\n\nX = df_train_opt.drop(columns='Target')\ny = df_train_opt['Target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_columns = X.select_dtypes(include='category').columns.tolist()\n\n# I ran the commented study below on a PC locally, but I use the resulting parameters in this Kaggle notebook. \n\n# n_trials = 100\n# Define the objective function for Optuna\n# def objective(trial):\n#     param = {\n#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.03, 0.5, log=True),\n#         \"iterations\": trial.suggest_int(\"iterations\", 100, 3000),\n#         \"depth\": trial.suggest_int(\"depth\", 1, 10),\n#         \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.1, 10, log=True),\n#         \"border_count\": trial.suggest_int(\"border_count\", 1, 500),\n#         \"random_strength\": trial.suggest_float(\"random_strength\", 0.1, 10, log=True),\n#         \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.01, 10, log=True),\n#         \"random_seed\": 1,\n#         \"eval_metric\": \"Accuracy\",\n#         \"loss_function\": \"MultiClass\",\n#         \"verbose\": False,\n#         \"cat_features\": cat_columns\n#     }\n\n#     # Train the model\n#     model = CatBoostClassifier(**param)\n#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)\n#     model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False, use_best_model=True)\n\n#     # Evaluate on the validation set\n#     y_val_pred = model.predict(X_val)\n#     accuracy_val = accuracy_score(y_val, y_val_pred)\n\n#     return accuracy_val\n\n\n# # Perform hyperparameter optimization with Optuna\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=n_trials, n_jobs=-1, show_progress_bar=True)\n# best_params = study.best_params\n# print(\"Best parameters:\", best_params)\n\n\n\n# Train the model on the full training dataset with the best hyperparameters\nbest_params = {'learning_rate': 0.12146914273449388, 'iterations': 2656, 'depth': 3, 'l2_leaf_reg': 0.8104011685674616, \n                  'border_count': 298, 'random_strength': 0.9482384798609886, 'bagging_temperature': 0.0814623508183759}   \nbest_model = CatBoostClassifier(**best_params, cat_features=cat_columns)\n\n\n# Evaluate on the validation set\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)\nbest_model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False, use_best_model=True)\ny_val_pred = best_model.predict(X_val)\naccuracy_val = accuracy_score(y_val, y_val_pred)\nprint('accuracy_val:', accuracy_val)\n\n\n\n# Make predictions on the test dataset\nbest_model.fit(X, y)\npredictions = best_model.predict(df_test_opt)\n\n# Reshape predictions array to be 1-dimensional\npredictions_flat = predictions.flatten()\n\n# Prepare the submission dataframe\nsubmission = pd.DataFrame({\n    'id': submission_id,\n    'Target': predictions_flat\n})\n\nprint('Submission head:', submission.head(10))\n\n# Save the submission dataframe to a CSV file\nsubmission.to_csv('catboost_optuna_submission_Classification_with_an_Academic_Success_Dataset.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}